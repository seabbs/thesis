---
output:
  html_document: default
  pdf_document: default
  word_document: default
---

# Fitting a dynamic transmission model of Tuberculosis {#model-fitting}


```{r model-fitting-setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, dpi = 320, 
                      fig.width = 8, fig.height = 8,
                      out.width = "80%", fig.align = 'center')

## Wordcount: 5501
##packages
library(tidyverse)
library(purrr)
library(kableExtra)
library(pander)
library(fs)
## Resources path
resources_path <- file.path("chapters", "model-fitting")


## Path to plots
plots_path <- file.path(resources_path, "plots")

## Read data: https://github.com/seabbs/ModelTBBCGEngland/data
load("chapters/model-development/data/measurement_params_table.rda")


## Read in model fits from https://github.com/seabbs/vignettes/results/final-fit
files <- fs::dir_ls("chapters/model-fitting/data")

load_rds <- function(file) {
  name <- basename(file) %>%
    stringr::str_replace("\\.rds", "")
  
  assign(name, readRDS(file), envir = globalenv())
}

walk(files, load_rds)

## Load Scenario DICs
load_rds("chapters/model-fitting/scenario_dics.rds")
```

## Introduction

In the previous chapter I outlined a mechanistic model of Tuberculosis (TB) transmission. Whilst this model made use of the best available evidence there remains a large degree of uncertainty regarding its structure and parameterisation. The majority of this uncertainty relates to the amount of TB transmission occuring in England. An approach to deal with this uncertainty is to fit the model to available observed data. Model fitting involves optimising over the available parameter space to return parameter sets that fit the data in some quantative way "better" than other parameter sets.  

This chapter details an approach to challenging a infectious disease model to data using the state-space model formulation and bayesian model fitting techniques. It first outlines the infectious disease model discussed in the previous chapter as a state-space model, as well as detailing the data used for fitting the model, and the parameters that are fitted. It then outlines the theoretical, and practical, justification for the model fitting pipeline used to calibrate and fit this state space model. Finally it discusses the quality of the model fit, ad-hoc techniques used to improve model fit, strengths and limitations of the approach, and areas for further work.

## Formulation as a state-space models

State space models (SSMs) may be used to model dynamic systems that are partially observed (such as all but the most contained infectious disease outbreak or endemic). They consist of a set of parameters, a latent continuous/discrete time state process and an observed continuous/discrete time process.[@Murray2015] The model developed in the previous section represents the state process of the SSM, with the parameters estimated for the model representing the model initial conditions and parameter set. To complete the mapping to an SSM an observational model is required. This observational model takes the latent estimates from the dynamic model and forecasts the observed data. We specifiy such an observational model in the Section \@ref(observational-model).

### Observed data {#observed-data}

Before the observation model can be detailed the data that will be used to fit the state-space model must be detailed.

The primary data source used was the reported UK born TB notifications from 2000 to 2004 as recorded by the Enhanced TB Survelliance (ETS) system (see Chapter \@ref(data)). These are the only years for which notifications are available stratified by UK birth status and for which universal school-age BCG vaccination was in place. As the model of TB transmission outlined in the previous chapter had multiple age-dependent parameters UK born TB notifications stratified by age group are also fitted to. The age groups considered are children (aged 0-14 years old), adults (15-69 years old), and older adults (70-89 years old). These age groups were used, rather than the more detailed 5 year age groups used in the TB transmission model (see Chapter \@ref(model-development)), for three reasons. Firstly, by using condensed age groups the number of notifications in each group increased. Having a larger number of notifications in each group reduces the impact of stochastics noise, which makes fitting the model easier. Secondly, reducing the number of data points, whilst still capturing the important age dynamics, reduces the compute requirements of the model (see Section \@ref(particle-filter)). As will be discussed in Section \@ref(fitting-pipeline) this was a major consideration as the model fitting approached used was highly compute intensive. Finally, the condensed age groups used were chosen to be meaningful. This allows model results to be more easily reported and interpreted. 

In addition to the notification data available via the ETS pulmonary TB notifications (including both UK born and non-UK born cases) from 1990, 1994, and 1998 were also fitted too. These TB notifications were made to Statutory Notifications of Infectious Diseases (NOIDS) from 1913 to 1999. These data were sourced from Public Health England,[@PHE2017] and made available in R using `tbinenglanddataclean`^[Historic TB notification data via `tbinenglanddataclean`: https://www.samabbott.co.uk/tbinenglanddataclean/]. Data from these time points was used as using data from the decade prior to the time period of interest allows the long term trends to be fitted to. A subset of the available data was used as this limited the impact on the compute time of the fitting pipeline. Additional data points would have brought only marginal improvements to the model fit (see Section \@ref(particle-filter)).

### Observational model {#observational-model}

There are three major considerations to account for when developing an observed disease notification model (i.e a reporting model). These are: systematic reporting error over time; systematic changes in reporting error over time; and reporting noise. For the notification data outlined in the previous section there is little evidence to suggest the form that this measurement model should take. For this reason I have assumed that all reporting errors are gaussian and that their are no time variable reporting errors. The reporting model can be then defined as follows,

$$O = \mathcal{N}\left(E_{\text{syst}}A, E_{\text{noise}}A\right)$$

Where $O$ are the observed notifications, $A$ are the incident cases of disease as forecast by the disease transmission model, $E_{\text{syst}}$ is the systematic reporting error, $E_{\text{noise}}$ is the reporting noise, and $\mathcal{N}$ represents the gaussian (normal) distribution. The priors for the model are defined in Table \@ref(tab:measurement-model). The prior for systematic reporting error is based on the assumption that cases are less likely to be reported than they are to be over reported. The prior for the reporting noise is an assumption. This observation model was also used for the non-UK born pulmonary cases. 

A potential limitation of this model is that reporting of TB cases is likely to have improved over time. This is especially true of notifications reported prior to the introduction of the ETS in 2000 (see Chapter \@ref(data)). A potential improvement to this model would be to introduce seperate systematic reporting errors for notifications pre and post the introduction of the ETS. However, given that I have only included a single point prior to the introduction to the ETS doing so in this instance would lead to overfitting of these parameters. Instead I have assumed that $ E_{\text{noise}}$ is doubled for data points prior to the introduction of the ETS.

```{r measurement-model}
measurement_params_table %>% 
  select(Parameter, Description, Distribution, Units, Method, Type) %>% 
kable(
  caption.short = "Measurement model parameters, descriptions, prior distributions, units, method used to derive the prior distribution and the type.",
  caption = "Measurement model parameters, descriptions, prior distributions, units, method used to derive the prior distribution and the type (i.e data derived, literature, assumption). $\\mathcal{U}$ = Uniform", booktabs = TRUE, longtable = TRUE, escape = FALSE) %>% 
  kable_styling(font_size = 8, latex_options = c("repeat_header")) %>% 
  column_spec(c(5), width = "6cm") %>% 
  column_spec(c(2), width = "4cm") %>%
  column_spec(c(3), width = "6cm") %>%
  column_spec(c(1,4,6), width = "1.5cm") %>% 
  landscape()
```

### Fitted parameters

The model outlined in the Chapter \@ref(model-development) has a large number of free parameters for which prior distributions have been specified based on the observed data, the literature, and expert knowledge. In theory the model fitting pipeline outlined below could be used to produce posterior distributions for all these parameters. However, in practise this is not feasible as the data discussed in Section \@ref(observed-data) only covers notifications and therefore does not contain sufficient information. If every parameter was allowed to update based on the data then it is likely that the resulting posterior distributions would not match with alternative data sources and the literature. Another potential issue is that by allowing all parameters to be fitted the meaningful information in the observed data may be lost. 

For this reason in the model fitting pipeline outlined here I have only allowed parameters relating to TB transmission, and measurement model parameters, to have their posterior distributions updated by the model fitting pipeline. All other parameters have posterior distributions that match their prior distributions. Parameters that have updated posterior distributions based on the data are,

* Mixing rate between UK born and non-UK born ($M$).
* Scaling on non-UK born cases ($\iota_{\text{scale}}$).
* Effective contact rate ($c_{\text{eff}}$).
* Historic effective contact rate ($c^{\text{hist}}_{\text{eff}}$).
* Half life of the effective contact rate ($c^{\text{hist}}_{\text{half}}$).
* Low risk latent activation rate modifier for older adults (70+) ($\epsilon^{\text{older-adult}}_L$).
* Systematic reporting error ($E_{\text{syst}}$).
* Reporting noise ($E_{\text{noise}}$).

In addition for scenarios with age variable transmission probabilities or non-UK born mixing the following paramters may also be fitted to,

* Transmission probablity modifier for young adults ($\beta_{\text{young adult}}$).
* Non-UK born mixing modifier for young adults ($M_{\text{young adult}}$).

## Model fitting pipeline {#fitting-pipeline}

### Introduction

Fitting dynamic transmisson models is complex and requires the use of specialist statistical techniques. There are a variety of these tools available. Ranging for tried and tested to cutting edge. Historically many modellers have used maximum likelihood methods to fit deterministic models. Unfortunately, whilst computationally efficient, these methods only provide point estimates and are limited to relatively simple models. More recently Bayesian methods have become popular. These have numerous benefits including: explicit inclusion of prior knowledge via prior distributions for all parameters; ability to handle complex stochastic models; and provide parameter distributions (posterior distribution) of best fitting parameters rather than single point estimates. Unfortunately these methods also require calibration prior to use. This section outlines the theoretical justification, and implementation details, of an automated model fitting pipeline used to calibrate, and fit, the previously detailed state space model of TB tranmission in England.

Libbi was used for all model fitting.[@Murray2015] LibBi is a software package for state-space modelling and Bayesian inference. It uses a domain specifc language for model specification, which is then optimised and compiled to provide highly efficient model code. It focusses on full information model fitting approaches including: particle Markov chain Monte Carlo (PMCMC), and SMC-SMC methods for parameter estimation. All fitting algorithms are highly efficient and scalable across mutliple CPUs or GPUs. `RBI` and `RBI.helpers` were used to interface with LibBi from R.[@Funk:2019ud; @Funk:2019uw] `RBI.helpers` was also used to optimise the model fitting pipeline as detailed in the calibration section. As model fitting using `LibBi` is compute intensive a workstation was built, and overclocked, with these compute requirements in mind^[See these blog posts for details: https://www.samabbott.co.uk/post/building-an-rstats-workstation/, https://www.samabbott.co.uk/post/benchmarking-workstation-xgboost/, https://www.samabbott.co.uk/post/benchmarking-workstation-benchmarkme/]. All model fitting code is available on GitHub as an R package^[See here for details: https://github.com/seabbs/ModelTBBCGEngland].

### The particle filter {#particle-filter}

In order to fit a model to data it is neccessary to estimate, or calculate, the marginal likelihood. Mathematically, the marginal likelihood is the plausibility that a parameter set, given the specified statistical model and the initial conditions, describes the observed data. For complex state space models, such as that discussed in the previous chapter, calculating the marginal likelihood is not possible.[@Murray2015] The particle filter provides a model-agnostic approach, based on importance sampling, to estimate the marginal likelihood. The variant used in this thesis, the bootstrap particle filter, is described below. See [@Murray2015] for a more technical discussion of the bootstrap particle filter.

1. For a given parameter set the particle filter is initialised by drawing a number of random samples (state particles) from the initial conditions of the model under consideration. These samples are then given a uniform weighting. 

2. Sequentially for the each observed data point the particle filter is then advanced through a series of *propagation*, *weighting*, and *resampling* steps. 
  
  * *Propagation*: For each particle the model is simulated, producing a forecast of the observed data data point.
  
 * *Weighting:* The likelihood of the new observation, given the predicted state, is then computed for each state particle. State particles are then weighted based on this likelihood.
 
 3. The marginal likelihood (likelihood of the observed data given the parameter set, marginalised across the initial conditions) can then be estimated by taking the product of the mean likelihood at each observed data point. A sample trajectory can also be calculated using the estimated weights from each time point.
 
### Sequential Monte Carlo

The particle filter approach outlined above, is a member of a family of sequential monte carlo (SMC) methods. These methods all initialise particles and then follow the same *propagation*, *weighting*, and *resampling* steps as previously detailed. SMC may also be used to sample from the posterior distribution of a given set of priors and a specified model. This works as follows,

1. Initially a number of samples (parameter particles) is taken from the prior distribution of the parameters and assigned a uniform weighting.

2. These parameter particles are then iterated sequentially over each observed data point, undergoing the same *propagation*, *weighting*, and *resampling* steps as in the particle filter, as well as an additional *rejuvenation* step.

   * *Propagation:* The model is simulated to the next observed data point. 
   
   * *Weighting:* Parameter particles are weighted using the marginal likelihood. In principle this could be computed exactly, but is most commonly estimated using a nested particle filter for each state particle (i.e as outlined in the previous section). For a subset of models a Kalman filter maybe used instead.[@Murray2015] The marginal likelihood may also be estimated using other partial information techniques such as approximate bayesian computation. In the case where a particle filter is used the full algorithm is known as Sequential Monte Carlo - Sequential Monte Carlo (SMC-SMC). This algorithm is used for all dynamic model fitting in this thesis. 

 *  *Resampling:* The parameter particle stock is restored to equal weights by resampling particles, with replacement, with the probability of each sample being drawn being proportional to its weight. 
 
 * *Rejuvenation:* *Resampling* of the parameter particles at each time point leads to a reduction in the number of unique values present. For state particles (when estimating the marginal likelihood using a particle filter) particles are diversifed with each propagation but as parameters do not change in time parameter particles cannot diversify in this way. To account for this the *rejuvenation* step is inserted after the resampling of parameter particles at each time point. The *rejuvenation* step is a single, or multiple depending on the acceptance rate, Metropolis-Hastings step for each parameter particle. This step aims to preserve the distribution of the parameter particles, whilst increasing their diversity. To minimise unnessary rejuvenation an effective sample size thresold can be used. This only triggers rejuveation when particle diversity has decreased below the target effective sample size thresold. 

#### Marginal Metropolis-Hastings

The Metropolis-Hasting step may be used as a model fitting approach in it's own right (MCMC) when repeated sequentially. It works by proposing a new value from the proposal distribution, estimating the marginal likelihood using the attached particle filter (or using any other exact or inexact method), and then accepting or rejecting the move based on the acceptance probability.[@Murray2015] Where the acceptance probability is given by,

$$ \text{min} \left(1,  \frac{p(y(t_{1:T}) |\theta')p(\theta')q(\theta | \theta')}{p(y(t_{1:T}) |\theta)p(\theta)q(\theta' | \theta)}\right) $$

Where $y$ is the observed data, $\theta$ is the current parameter set, $\theta'$ is some proposed parameter set sampled from some proposal distribution $q(\theta' | \theta)$.[@Murray2015] By construction samples drawn using this rule are ergodic to the posterior distribution. This means that after convergance samples drawn using this rule may be considered as samples from the posterior distribution. 

### Calibration

#### Particle calibration

The accuracy of the marginal likelihood estimate returned by the particle filter is dependent on the number of particles used, the number of observed data points, the parameter sample, and the complexity of the model. As the number of particles tends towards infinity the likelihood estimate provided by the particle filter should tend towards the exact solution. This suggests that choosing a very high number of particles maybe the optimal solution in terms of accuracy. Unfortunately, each particle requires a full model simulation, which for complex models can be computationally costly. This means that using very large numbers of particles is not tractable. For this reason it is neccassary to determine an optimal number of particles that both provides an adequately accurate estimate of the likelihood whilst being computationally tractable. 

The `rbi.helpers` R package attempts to solve this issue by adopting the following strategy.[@Funk:2019uw] First, the approximate mean of the posterior distribution is obtained, as accurate likelihood estimates near the posterior mean are of the most interest. Repeated model simulations are then run using the same set of parameters, with the marginal likelihood being estimated each time using a given number of particles. The variance of these log-likelihood estimates is then calculated. This process is then repeated for increasing numbers of particles until the log-likelihood variance is below some target thresold, commonly 1.[@Funk:2019uw]

I have implemented this as a two step process for each fitted scenario. Firstly, I used the Nelder-Mead simplex method, via LibBi,[@Murray2015] to find a parameter set that optimised the maximum likelihood. I then initialised a 1000 step PMCMC chain with this parameter set, using 256 particles in the particle filter. I then used `rbi.helpers`,[@Funk:2019uw] as outlined above, to estimate the number of particles required to produce a log-likelihood variance of less than 1 for this sample of the posterior distribiution, using 250 samples per step and starting with 16 particles. I initially planned to repeat this process for multiple draws from the posterior distribution but this proved to be infeasible given the compute available. A target of 5 for the log-likelihood variance was chosen as a smaller target could not be feasibly achieved given the compute resources available. Additionally 256 was specified as the maximum number of feasible particles to use in the particle filter. 

#### Proposal calibration

When using an MCMC alogorithm a proposal distribution is required to provide new parameter samples to evaluate. For SMC-SMC a proposal distribution is required to inform the MCMC sampler that is run during the rejuvernation step. By default if no proposal distribution is provided Libbi uses the prior distribution.[@Murray2015] The prior distribution can be an inefficient proposal distribution as it is likely to have a low acceptance rate (from the MCMC sampler).[@Murray2015] Having a low acceptance rates means that many more MCMC steps are required to generate a successful parameter sample. This results in slow mixing and computationally expensive MCMC steps may make model fitting intractable.

A more efficient approach is to specify a proposal distribution that draws parameter samples that are closer to the current state of the MCMC chain than the overall prior distribution. There is an extensive literature examing how to optimise the proposal distribution to achieve an good acceptance rate. In practise it has been shown that a rate of between 10% and 20% is optimal for upwards of 5 parameters.[@Murray2015] This strikes a balance between allowing the chain to fully explore the posterior distribution whilst still being as efficient as possible.

A simple approach to setting the proposal is to run a series of MCMC steps and then calculate the acceptence rate. Based on the acceptence rate the width of the proposal distributions can then be adapted. By repeating these steps multiple times a proposal distribution which gives an acceptance rate within the desired bounds can be arrived at. This adaption can either be independent for each parameter or dependent (taking into account empirical correlations). The `adapt_proposal` function, from the `rbi.helpers` R package,[@Funk:2019uw] implements this approach and is used in this model fitting pipeline. In many models parameters are likely to have strong correlations (i.e between UK and Non-UK born mixing rate and effective contact rate) and in these scenarios it is likely that a dependent strategy for adapting the proposal distribution will more efficiently explore the posterior distribution. However, the downside of adapting the proposal distribution using dependent methods is that the resulting proposal is highly complex, is computationally expensive to compute and may breakdown in some areas of the posterior distribution.

In this model fitting pipeline I have used a maximum of 5 iterations of, manual, independent proposal adaption, drawing 250 samples in each iteration, starting with guassian distributions for each parameter, truncated by the range of the prior, with the mean based on the current parameter value. The standard deviation for each parameter was assumed to be the standard deviation of the prior if it was guassian and otherwise assumed to be the range of the prior if it was uniform. For each iteration I halved the  size of the standard deviation of each parameter. As for the particle calibration, I initially used a maximum likelihood method to provide a point estimate of the best fitting parameter set, followed by 1000 PMCMC steps, using a 256 particle filter. This means that the proposal distribution is adapted near to the posterior mean rather than in the tails of the posterior distribution. 

I chose to use, manual, independent proposal adaption method for several reasons. Firstly, when developing this pipeline the approaches implemented in `rbi.helpers` produced multiple transient errors in other `rbi` and `rbi.helpers` code. Secondly, the resulting dependent proposal distribution was highly complex, slow to compute, and diffult to debug. Finally, for SMC-SMC efficient exploration of the proposal distribution is less important than when using MCMC alone as SMC-SMC is initialised with multiple samples from the prior distribution. This means that multiple local maximas can be efficiently explored regardless of the proposal distribution used. The MCMC rejuvernation step then serves to provide additional samples from these local maximas. Proposal adaption was only carried out for the main model scenario with all other scenarios using this proposal distribution. 

### Model comparision

In the previous chapter multiple potential model structures were outlined, each of which could be valid based on theoretical considerations. The observed data can be used to identify which of these structures best reflects reality. This can be done using the deviance information criterion (DIC). The DIC is a hierarchical modeling generalization of the Akaike information criterion (AIC) and can be used to compare nested models.[@Gelman:nll_LBlw] 

Smaller DIC values should indicate a better fit to data than larger DIC values. The DIC is composed of the deviance, which favours a good fit, and the effective number of parameters, which penalises overfitting.[@Gelman:nll_LBlw] Unlike the AIC the DIC can be estimated using samples from the posterior distribution and so is more readily calculated for models estimated using bayesian methods. It can be defined as,

$$ {\mathit  {DIC}}=D({\bar  {\theta }})+2p_{D}$$
Where $\bar{\theta}$ is the expectation of $\theta$, with $\theta$ being defined as the unknown parameters of the model. $p_{D}$ is the effective number of parameters in the model and is used to penalise more comple complex models. It can be estimated as follows,[@Gelman:nll_LBlw]


$$p_{D}=p_{V}={\frac  {1}{2}}\widehat {\operatorname {var}}\left(D(\theta )\right).$$

Finally the deviance is defined as,

$$D(\theta)=-2\log(p(y|\theta ))+C$$

Where y are the data, $\displaystyle p(y|\theta)$ is the likelihood function and C is a constant. C cancels out when comparing different models and therefore does not need to be calculated. 

The DIC has two limitations. The first of these is that in its derivation it is assumed that the model that generates future observations encompasses the true model. This assumption may not hold in all circumstances. The second limitations is that the observed data is used to construct both the posterior distribution and to estimate the DIC. This means that the DIC tends to select for over-fitted model.[@Gelman:nll_LBlw] 

In this chapter I have used the DIC, as estimated by the `DIC` function from `rbi.helpers`,[@Funk:2019uw]  to evaluate the various model structures outlined in the previous chapter. 

### Parameter sensitivity

Understanding the impact of parameter variation can help when interpreting findings from a model, targeting interventions, and identifying parameters for which improved estimates are needed. Often parameter sensitivity is assessed using single-parameter or local sensitivity analyses. Unfortunately, these techniques do not accurately capture uncertainty or sensitivity in the system as they hold all other parameters fixed.[@Marino2009a] Various techniques exist that can globally study a multi-dimensional parameter space. In this section, I will outline the partial rank correlation coefficient method (PRCC) and discuss it's strengths and weaknesses. The implementation of PRCC to estimate the parameter sensitivity of parameters fitted using the model fitting pipeline presented above is then discussed.


PRCC is a sampling based approach which can be computed with minimal computational cost from a sample of the prior or posterior distributions of a model. It estimates the degree of correlation between a given parameter input and an output after adjusting (using a linear model) for variation in other inputs. It is an extension of more simplistic sampling techniques, the most basic of which, is simply examining scatter plots of a sampled parameter set against the outcome of interest. PRCC is required as these more simplistic techniques become intractable with higher dimensionality as they do not account for between paremeter correlation or are just difficult to interpret with multiple dimensions.[@Marino2009a] PRCC can be understood by first outlining the individual steps. These are:

1.  **Correlation:** Provides a measure of the strength of a linear association between an input and and output (scaled from -1 to 1). It is calculated as follows,

\[ 
{\displaystyle \rho _{X,Y}={\frac {\operatorname {cov} (X,Y)}{\sigma _{X}\sigma _{Y}}}}
\]

Where $\operatorname {cov}$  is the covariance, $\sigma _{X}$ is the standard deviation of $X$, and $\sigma_Y$  is the standard deviation of $Y$. Where $X$ is the input and $Y$ is the output.

2. **Rank Correlation:** This is defined as for correlation but with the data being rank transformed. Rank transformation reorders inputs and outputs in magnitude order. Unlike non-rank transformed correlation it can handle non-linear relationships but still requires monotonicity. 

3. **Partial Rank Correlation:** Inputs and outputs are first rank transformed as above. Linear models are then built which adjust for the effects of the other inputs on $Y$, and on the current input $X_i$. Correlation is the calculated as above using the residuals from these models.

A limitation of PRCC is that it whilst it can capture non-lnear relationships between outputs and inputs these relationships must be monotonic.[@Marino2009a] For relationships that are non-monotic methods that rely on the decompositon of model output variance, such as the extended Fourier amplitude sensitivity test,[@Marino2009a] are more appropriate. However, these approaches are computationally demanding as they typically require mutliple iterations of model simulation. Additionally, they cannot be used on a previous parameter samples, instead needing to sample and simulate the model within the parameter sensitivity algorithmn. This means that they cannot be used for "free" (i.e with neglible additional compute cost), unlike PRCC which can be estimated using a sample from the posterior distribution. For this reason these approaches have not been further explored in this thesis. 

I have implemented PRCC using the `epiR` R package^[Code: https://github.com/seabbs/ModelTBBCGEngland/blob/master/R/test_sensitivity.R],[@EpiR] using the samples from the posterior distribution of the model calculated during the SMC-SMC step. Parameter sensitivity measures such as PRCC must be calculated seperately for each output time point. I calculated the PRCC for each fitted parameter, at the final time point with fitted data (2004), for overall TB incidence rates. These results are then summarised by plotting the absolute PRCC values, indicating the direction of correlation using colour^[Code: https://github.com/seabbs/ModelTBBCGEngland/blob/master/R/plot_sensitivity.R]. 

### Pipeline overview

The full model fitting pipeline can be summarised as follows^[Code: https://github.com/seabbs/ModelTBBCGEngland/blob/master/R/fit_model.R]:

1. Model initialisation using minimal error checking and single precision computation. Implemented using the `disable-assert` adn `enable-single` flags in LibBi.[@Murray2015] Outputs are only given for times with observed data and a subset of parameters are recorded for final reporting^[Model code: https://github.com/seabbs/ModelTBBCGEngland/blob/master/inst/bi/BaseLineModel.bi]. 

2. 1000 parameter sets were taken from the prior distribution and the model was then simulated for each one. 

3. Maximum likelihood optimisation with 100 steps, using the Nelder-Mead simplex method, via LibBi.[@Murray2015] This approximates the mean of the posterior distribution.

4. 1000 PMCMC steps, with 256 particles used in the particle filter. This provides a better estimate of the mean of the posterior distribution.

5. Particle adaption via `rbi.helpers` at the approximate mean of the posterior distribution.[@Funk:2019uw] A minimum of 64 particles and a maximum of 256 particles are assessed with the target of a log-likelihood variance of less than 5. 250 PMCMC steps were used at each stage to estimate the log-likelihood variance.

6. Manual independent proposal adaption at the approximate mean of the posterior distribution. It is assumed that the proposal for each parameter is guassian, truncated by the range of the prior with the mean based on the current parameter value. The standard deviation of the proposal distribution is halved each iteration, with at most 5 iterations of adaption. The minimum target acceptance rate specified was 10% and the maximum was 20%. 250 PMCMC samples were used each time to estimate the acceptance rate. Proposal adaption was only carried out for the main model scenario, with other scenarios using this proposal. 

7. SMC-SMC model fitting with 1000 initial parameter particles. Particle rejuvernation was set to trigger when the effective sample size decreased below 25%, with 10 MCMC steps used each time. 

8. For each sample from the posterior distribution the model was then simulated for all time points.

9. The model DIC was computed using `rbi.helpers`.[@Funk:2019uw] This gives a model agnostic approach to evaluate the fit to the observed data.

10. Parameter sensitivity was estimated by calculating the partial rank correlation coffecient (PRCC) for each model parameter, for the final time point fitted too (2004), for overall TB incidence rates. Results were then plotted in order of the absolute magnitude of the correlation, with the direction of the correlation determined using colour. The `epiR` package was used to compute the PRCC.[@EpiR] 

## Results

The pipeline outlined above resulted in a poor fit to the observed data. The SMC-SMC algorithm had a low effective sample size for each iteration, and a low acceptance rate for particle rejuvernation steps in all scenarios evaluated. This resulted in spuriously tight priors. Ad-hoc calibration (detailed in the following section) failed to improve the quality of this fit or find a subset of the model - or parameters - that fit the observed data to an acceptable degree whilst remaining computationally feasible. The results presented in the following section are therefore preliminary and indicative only. 

### Ad-hoc calibration

Minor alterations to the model fitting pipeline had little impact on the quality of the fit. To attempt to improve the quality of the model fit I used a combination of ad-hoc approaches. As a first step I introduced a calibration model with variation allowed only in the fitted parameters with all other parameters using point estimates. This allowed a reduced number of particles to be used to estimate the marginal likelihood and hence dramatically decreased compute cost and runtimes. This reduced model was then used for the following tests:

* Increasing the number of particles used in the outer SMC loop at the expense of reducing the number of particles used for marginal likelihood estimation.
* Increasing the number of particles used for marginal likelihood estimation. 
* Sequentially decreasing model complexity by fixing fitted parameters to manually tuned point estimates.
* Increasing the number of parameters fitted to rather than used as fixed distributions.
* Varying the  size of the proposal distribution, rejuvernation thresold, number of particles and rcejuvernation steps.
* Varying the fitted observed data. This took 3 main forms:
    * Aggregation: fitting to overall incidence only; fitting to incidence grouped by large age groups (i.e children, adults, older adults);  fitting to only age groups of interest (i.e children). 
    * Reducing the timespan of the fitted data. This included simplifying down to a single year of data but also included using various combinations of time points.
    *  Exploring additional observed data. This included attempting to fit to observed pulmonary TB case from 1980 onwards. This approach sought to constrain the parameter space to give a more realistic age distribution of cases. 
* Changing the functional form for the decay in the historic contact rate. This involved exploring linear decay with the decay gradient dictated by the year that the current contact rate takes effect and the year that the historic contact rate begain to decay.
* Exploring using time dependent modifiers on the transmission probability and non-UK born mixing. 
* Exploring using modifiers for children, adults, and older adults for both the transmission probability and non-UK born mixing. This was essentially an extension of the original scenarios considered using the model fitting pipeline. 
* Exploring widening and narrowing the prior distributions of fitted parameters beyond realistic ranges. 
* Exploring varying the size of the initial high, and low, risk latent populations. This included starting with no latent cases, starting with a reduced proportion of latent cases and starting with a much larger latent population to simulate a historically more widespread disease.


None of these approaches dramatically improved model fit to the point that more robust inference could be drawn. Reducing the number of parameters, and time points, fitted too decreased the computational cost of the pipeline and improved acceptance rates. However, model fits remained poor until a single timepoint and aggregated incidence were fitted too using a single varying parameter (effective contact rate) with all others manually tuned. Unfortanately, this simplifed the model to the extent that it could not be used to generate meaningful results. The introduction of multiple timepoints led to poor model fits regardless of the observational data used. This effect may be attributed to particle degradation but was not resolved by the addition of more particles in either the marginal likelihood estimation or the outer SMC step.[@Murray2015] The use of pre-ETS pulmonary TB data worsened the quality of the model fit. This may be attributed to the data including UK born cases and therefore making the model fit more sensitive to the assumption used for the number of non-UK born notifications prior to 2000. Using manual prior tuning, transmission and mixing modifiers allowed a relatively close fit to the observed data but additional parameters, beyond those specified in the model fitting pipeline did little to improve on this. The quality of the model fit using the model fitting pipeline was poor regardless of the number of modifier parameters used. Varying the initial latent populations resulted in higher than previously estimated historic effective contacts but again did little to improve the quality of the model fit. 

[@Murray2015] suggests that increasing the number of particles used in SMC may improve the quality of the model fit. Unfortunately as `LibBi` stores SMC particle paths in Random access memory (RAM) the number of particles was restricted. An additional limitation is that each rejuvernation steps also need to be stored in RAM. Attempts to increase the amount of avaialble RAM (64 GB) using a 500GB SWAP (virtual memory) drive increased the upper limit on the number of particles but gains from this were restricted due to thrashing^[Thrashing: https://en.wikipedia.org/wiki/Thrashing_(computer_science)]. Thrashing occurs when too much data is written to SWAP memory in a short period of time and usually results in a system crash.  A major cause of the high RAM requirements of the model was that `LibBi` stores all parameters defined as initial conditions across every time point in the model, regardless of the settings used. Attempting to reduce the RAM footprint by fitting a greater number of parameters allowed for a greater number of particles to be used but also increased the model degrees of freedom and hence upped the required number of particles required by a greater amount than provided by fitting the parameter. Reducing the number of particles used to estimate the marginal likelihood allowed a greater number of particles to be used in the outer SMC step but this resulted in highly inaccurate estimates of the likelihood. These highly inaccurate likelihood estimates resulted in the SMC-SMC algorithm focussing on parameter sets that had a low likelihood estimate yet fit the data poorly - ultimately leading to even poorer model fits. 

Varying the proposal size, rejuvernation thresold, and number of rejuvernation steps showed some promise at improving the quality of the model fit but ultimately computational constraints limited how much progress could be made using this approach. It is possible that a much longer runtime could result in an improved fit to data with no other model changes. Alternatively these results may driven by the model being too complex for an SMC-SMC approach to be viable in this framework.  

### Particle and proposal calibration

After development of the model fitting pipeline but before results could be produced both the `optimise` (from `rbi`) and `adapt_particle` (from `rbi.helpers`) began to error with a difficult to debug `LibBi` error message. This meant that steps 3-5 of the model fitting pipeline could not be used with the final model. As a work around the maximum permitted number of particles (256) was used for all model fitting (increasing the number of particles was also explored as detailed in the previous section). 

As discussed, there were multiple issues with fitting the final model and this made it diffult to determine what the mean of posterior distribution was. This made manually tuning the proposal distribution challenging and so instead a standard deviation of 1% was used. This value was chosen as it increased the acceptance rate by limiting each rejuvernation step to a relatively small subset of the prior distribution whilst not preventing the exploration of new parameter space in scenarios when model fits from the initial particle sample were poor.

### Model comparision

* Compare Scenarios using DIC and discuss results.

```{r 09-scenarios-table}
tibble(Scenario = names(scenario_dics) %>% str_split("_") %>% 
         map_chr(~paste(., collapse = " ")), 
       DIC = unlist(transpose(scenario_dics)$dic)) %>% 
  arrange(DIC) %>% 
  mutate(Scenario = Scenario %>% 
           str_replace("plus", "and") %>% 
           str_replace("baseline", "Baseline") %>% 
           str_replace("transmission", "Transmission variable in young adults (15-29)") %>% 
           str_replace("mixing", "non-UK born mixing variable in young adults (15-29)") %>% 
  str_to_sentence()) %>% 
mutate(DIC = round(DIC, 0)) %>% 
kable(
  caption = "DIC values for each scenario evaluated during model fitting - arranged from best to worst model fit.",
  booktabs = TRUE, longtable = TRUE) %>% 
  kable_styling(latex_options = c("hold_position")) %>% 
  column_spec(c(1), width = "10cm")
```

### Model Fit to TB incidence rates from the ETS

* Comment on fit

```{r 09-table-incidence-preds, message = FALSE, warning = FALSE}
sum_obs_table %>% 
  set_names(., nm = c(names(.)[-ncol(.)], "Predicted Incidence (95% CI)")) %>% 
  select(-Observation) %>% 
kable(
  caption.short = "Observed versus predicted overall TB incidence for years that the model was fitted too.", 
  caption = "Observed versus predicted overall TB incidence for years that the model was fitted too. The model underpredicted incidence for all years fitted too. The upper credible interval of the model predictions was substantially lower than the observed data. (95\\% CI): 95\\% credible interval estimated using the 2.5\\% quantile and the 97.5\\% quantile.", booktabs = TRUE, longtable = TRUE) %>% 
  kable_styling(latex_options = c("hold_position")) %>% 
  {.}
```

```{r 09-age-strat-incidence, fig.scap = "Observed and predicted TB incidence stratified by model age group", fig.cap = "Observed and predicted TB incidence stratified by model age group (0-11). 0-9 refers to 5 year age groups from 0-4 years old to 45-49 years old. 10 refers to those aged between 50 and 69 and 11 refers to those aged 70+. The model underpredicted incidence in all age groups fitted too with older adults have the best fit."}
knitr::include_graphics(file.path(plots_path, "overview-states-1990-age-1.png"))
```

### Posterior parameter distributions

* Comment on posteriors

```{r 09-fig-prior-params, fig.scap = "Prior and posterior distributions for fitted model parameters.", fig.cap = "Prior and posterior distributions for fitted model parameters. Parameter names in figure are in their coded form. They can be interpreted using the following:  $\\beta_{\\text{young adult}}$,$c_{\\text{eff}}$, $c^{\\text{hist}}_{\\text{eff}}$, $c^{\\text{hist}}_{\\text{half}}$, $M$, $M_{\\text{young adult}}$, $E_{\\text{syst}}$, $E_{\\text{noise}}$, $\\iota_{\\text{scale}}$, and $\\epsilon^{\\text{older-adult}}_L$"}
knitr::include_graphics(file.path(plots_path, "prior-posterior-overview-1.png"))
```


```{r 09-prior-posterior, warning = FALSE, message = FALSE}
sum_prior_posterior %>% 
  set_names(., nm = . %>% 
              str_replace("or", "or (95\\\\%CI)")) %>%
  mutate(Parameter = latex_param) %>% 
  select(-latex_param) %>% 
kable(
  caption.short = "Observed versus predicted overall TB incidence for years that the model was fitted too.", 
  caption = "Observerd versus predicted overall TB incidence for years that the model was fitted too. The model underpredicted incidence for all years fitted too. The upper credible interval of the model predictions was substantially lower than the observed data. (95\\% CI): 95\\% credible interval estimated using the 2.5\\% quantile and the 97.5\\% quantile.",
  booktabs = TRUE, longtable = TRUE, escape = FALSE) %>% 
  kable_styling(latex_options = c("hold_position")) %>% 
  {.}
```

### Parameter Sensitivity

* Comment on parameter sensitivity in light of strength of data

```{r 09-fig-sens, fig.scap = "Partial rank correlation coefficients for each parameter fitted too.", fig.cap = "Partial rank correlation coefficients for each parameter fitted too. Parameter names in figure are in their coded form. They can be interpreted as the following parameters:  $\\beta_{\\text{young adult}}$, $c_{\\text{eff}}$, $c^{\\text{hist}}_{\\text{eff}}$, $c^{\\text{hist}}_{\\text{half}}$, $M$, $M_{\\text{young adult}}$, $E_{\\text{syst}}$, $E_{\\text{noise}}$, $\\iota_{\\text{scale}}$, and $\\epsilon^{\\text{older-adult}}_L$", out.width = "200px"}
knitr::include_graphics(file.path(plots_path, "incidence-sensitivity-1.png"))
```

## Discussion


### Summary

In this chapter I have formulated the disease transmission model developed in the previous chapter as a state-space model, developed a model fitting pipeline to fit this model to observed data, discussed the approaches taken to try and improve the quality of the model fit, and presented preliminary results from the model fitting pipeline.


As dicussed earlier in this chapter, SMC-SMC was used for all model fitting. This uses an external SMC step to estimated the posterior distribution as well as an internal SMC step to estimate the marginal likelihood. In general bayesian model fitting approaches are beneficial as they allow prior information to be fully incorperated into model fitting and they produce posterior distribution estimates rather than point estimates.[@Murray2015; @Toni2009] There are two main families of approaches, SMC and MCMC, both previously discussed in this chapter. Using SMC to estimate the posterior distribution has numerous advantages over MCMC. The first of these is that MCMC aproaches are sensitive to their initial conditions. If a model has multiple local best fits MCMC may only converge to a single minima rather than fully exploring the posterior distribution. Multiple MCMC chains may be used to try and account for this but as each chain must be independently run to convergance only a few concurrent chains are likely to be practical. SMC on the other hand is initialised with a large sample from the prior distribution, meaning that local minimas are more likely to be explored. Parameter particle weighting and resampling then balances the contribution to the posterior distribution of these local minimas based on their fit to the observed data. Additionally, MCMC approaches are by definition sequential,[@Murray2015] although if they make use of particle filters these can be run in parallel. Increasing the number of particles in a filter may lead to an increase in the chain mixing rate of the MCMC chain but as particles numbers are increased any returns will decrease. To account for this multiple chains are often used, but as outlined above the burn-in required for each chain limits the potential speed-up. In comparision, each SMC parameter particle can have it's marginal likelihood computed seperately. Although the resampling step remains a bottleneck as it can only be completed once all marginal likelihoods have been computed. On the other hand SMC is less interative than MCMC meaning that model fitting is harder to inspect when it is in progress. This is because SMC is not sequential, unlike an MCMC run for which each draw can be inspected as it is computed. Similarly, as SMC is not a sequential technique multiple runs cannot be combined. This means that model fitting must be done in a single run using a priori knowledge to judge the number of MCMC rejuvernation steps required, and the expected total run time. SMC will also have a variable run time based on the effective sample size as rejuvernation only happens when parameter particles have been depleted beyond a certain point. An additional benefit of SMC approaches are that they can theoretically be extended to model selection as well as parameter posterior estimation - effectively estimating posteriors for candidate model structures.[@Toni2009] Beyond SMC and MCMC there are multiple other model fitting algorithms that each have their own strengths and benefits - discussion of these is outside the scope of this thesis. 

The particle filter has been shown to provide an unbiased estimate of the likelihood for arbitary state-space models.[@Murray2015] As a full information technique the particle filter provides a more accurate estimate of the likelihood than other approximate techniques, with relatively little tuning or user interaction.[@Murray2015] The major downside of the particle filter, compared to other approaches, is the high compute requirements, with each particle requiring a full model simulation. For highly complex models, the particle filter approach may not be tractable or a reduced level of accuracy of the marginal likelihood estimate must be accepted. In addition, the bootstrap particle filter may become depleted (i.e variance between particles is reduce to such an extent that the effective sample size becomes small) when the model being fitted is a poor fit for the observed data.[@Murray2015] Whilst this can be resolved using additional particles, or rejuvernation steps, this may not be computationally tractable. There are several alternative particle filters that seek to mitigate these issues but many of them are highly complex and do not significantly reduce the required compute - see [@Murray2015] for a detailed discussion of some of these alternatives. An alternative approach to the particle fitler is to use an approximate technique, such as approximate bayesian computation (ABC). ABC can be used to estimate the likelihood by comparing observed and simulated data.[@Toni2009] This dramatically reduces the required compute as multiple model simulations per parameter set are no longer required. The comparison between the observed and simulated data is facilitated using a distance function with parameter sets being accepted if they are within some thresold distance. This thresold can be be tuned to produce a good estimate of the posterior distribution. Developing a distance function for all of the observed data can often be challenging so instead the distance is often calculated using a set of summary features.[@Toni2009] In principle, ABC can be used with a wide variety of algorithms (including a simple rejection approach, MCMC, and SMC) to estimate the posterior distribution but in practise it has been shown that ABC-SMC generally performs better than alternative ABC approaches.[@Toni2009] The two major limitations of ABC compared to the use of a particle filter is that in most cases summary statistics must be used rather than calculating the distance from the observed data and a function for calculating the distance must be chosen.[@Toni2009; @Lintusaari2016] Whilst some techniques exist for evaluating summary statistics the choice is often subjective, relying on domain knowledge.[@Lintusaari2016] Chosen summary statistics rarely capture the information contained in the observed data fully and can inflate posterior distribution and in the worst cases introduce bias to the estimates.[@Busetto2013] The choice of distance function may also incluence the estimates of the posterior distribution.[@Lintusaari2016] 

This chapter showcased the use of `LibBi`, `rbi`, and `rbi.helpers` for fitting a complex transmission model. Unfortunately, the quality of the model fit was poor and the complexity of `LibBi` makes understanding the root cause difficult. It is possible that the model developed in the last chapter is too complex to be fitted using this approach - at least without the use of several orders of magnitude greater compute resources (or compute time) than available.[@Murray2015] However, it is also possible that even with these resources this pipeline may not have produced a high quality model fit as the model developed in the previous chapter was clearly much more complex than those envisioned by the developers of the software.[@Murray2015] Indeed the model itself may not be identifiable with the available data.[@Lintusaari2016] `LibBi`, `rbi` and `rbi.helpers` have great potential as a standardised toolbox for modellers. However, in their current state they are difficult to use beyond relatively simple use cases. This difficulty is compounded by sporadic, inconsistent, documentation of both the underlying software and it's `R` interface. On top of this, both `LibBi` itself and the R libraries that support it have multiple apparent bugs that can be extremly diffult to debug due to the layered nature of the software. `rbi` and `rbi.helpers` are under development and it is likely that many of these issues will be dealt with over time. However, `LibBi` itself has not had a major release since 2016, the community around it is largely inactive, and it's complex nature makes it diffcult for newcomers to contribute towards its development. 

There are many other tools available within the `R` ecosystem for fitting infectious disease models and detailing them all is beyond the scope of this thesis. However, `LibBi`, `rbi`, and `rbi.helpers` represent an attempt to provide a complete modelling framework rather than being a simple toolbox for model fitting. The `pomp` package has a similar aim making a comparison worthwhile.[@King2016] `pomp` defines models using a similiar structure to that presented here and used within `LiBbi`. However, rather than using its own modelling language it relies on the use of either `R` or `C` code. This has several advantages in that `pomp` models can more easily be generalised, can be understood and implemented by users new to the package and can make use of packages from the wider `R` and `C` ecosystem. The downside to this approach is that for complex models the use of `C` is essentially a requirement for efficiency reasons and implementing complex models in `C` can be an error prone and time consuming process. `pomp` offers support for PMCMC, iterated filtering, and ABC-MCMC but does not support SMC based aproaches (such as SMC-SMC and ABC-SMC).[@King2016] In developing the work presented here, and in the previous chapter, a simpler model was developed using `pomp`. This work was not included in this thesis as it was not sufficiently developed. However, it highlight similar issues with the `pomp` package to those observed when using `LibBi`. Whilst `pomp`'s documentation, stability, and testing were much improved over `LibBi` it had similar limitations when it came to complex models. This was to such an extent that I developed numerous helper functions to deal with both the input and output of `pomp` models^[idmodelr: https://github.com/seabbs/idmodelr]. `pomp`'s documentation also has a heavy focus on iterated filtering over other model fitting techniques. 

### Futher work

Whilst the results presented here are not encouraging it may be the case that with a greatly extended runtime a much better fitting model may be found. The first step to testing this is to rerun the pipeline using several orders of magnitude more rejuvernation steps - as these have no RAM overhead. Even if the resulting model fits are still poor this may indicate areas for improvement - in either the model fitting pipeline or the model itself. An alternative option is to recode the model developed in the last chapter into a more generic form (i.e C) and to attempt to fit the model using other techniques that are less compute intensive and more robust - such as SMC-ABC. Another option would be to reimplement the model in the form required by another modelling package - such as `pomp`. However, this would mean running the risk of again getting stuck within a framework that is difficult to debug and may not be working as expected. A final option would be to reduce the complexity of the model and potentially fit to less complex data. This may be the only solution if the current model is not identifiable.  

### Conclusions

The model fitting pipeline developed here was theoretically robust, and highly reproducible, but in practise did not produce a high quality model fit. It is difficult to determine whether this was caused by the complexity of the model combined with the high compute requirements of SMC-SMC or if the software implementation itself was at fault. However, `LibBi`'s high barrier to entry and difficulty of use made both implementing the model, and assessing whether the model fitting was working as expected more difficult. It is likely that a more generic model implementation coupled with a less compute intensive fitting approach would produce more useful results. This work does still have some merit as it pushed both `LibBi` and SMC-SMC to it's limits helping to define what the limitations of this approach, and specialised modelling packages more generally, may be. It is possible that with an extended runtime model fits may become more reliable and hence more usable.

## Summary

* Defined the disease transmission model from the previous chapter as a state-space model. 
* Outlined the observed data and defined an appropriate measurement model for this data. 
* Developed a model fitting pipeline based on SMC-SMC to fit the previously defined state-space model to TB notification data.
* Discussed ad-hoc approaches used to improve model fit. 
* Evaluated the various scenarios discussed in the previous chapter using DIC and discussed the implications of these findings. 
* Presented the model fit to data from the best fitting scenario as established using the DIC. 
* Discussed the posterior distributions of the fitted parameters and compared these distributions to the prior distributions.
* Estimated the model sensitivity to parameter changes and discussed the impact that this sensitivity may have.
* Discussed the strengths and limitations of the work presented here, as well as outlining potential further work. 

